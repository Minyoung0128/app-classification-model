{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from scapy.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/myk/min0/packet_tokenizer.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "ip_length = 20\n",
    "protocol_length = 60\n",
    "def get_header_value(packet):\n",
    "    ip_header_len=min(ip_length,packet[IP].ihl*4)\n",
    "    ip_byte = bytes(packet.getlayer(IP))[:ip_header_len]\n",
    "    \n",
    "    if packet.haslayer(UDP):\n",
    "        # UDP\n",
    "        udp_byte = bytes(packet[UDP])[:8]\n",
    "        protocol_byte = udp_byte\n",
    "        \n",
    "    elif packet.haslayer(TCP):\n",
    "        # TCP\n",
    "        tcp_header_length = len(packet[TCP]) - len(packet[TCP].payload)\n",
    "        tcp_byte = bytes(packet[TCP])[:tcp_header_length]\n",
    "        protocol_byte = tcp_byte\n",
    "        \n",
    "    elif packet.haslayer(ICMP):\n",
    "        icmp_byte = bytes(packet[ICMP])[:8]\n",
    "        protocol_byte = icmp_byte\n",
    "        \n",
    "    else:\n",
    "        print(\"no layer\")\n",
    "        return\n",
    "    \n",
    "    return ip_byte + protocol_byte\n",
    "\n",
    "def packet2string(packet):\n",
    "    bytes = get_header_value(packet)\n",
    "    \n",
    "    if bytes is None:\n",
    "        return\n",
    "    \n",
    "    byte_string=''\n",
    "    for byte in bytes:\n",
    "        byte_string +=('00'+hex(byte)[2:])[-2:]\n",
    "    return byte_string\n",
    "    \n",
    "def pcap2token(pcap_path, seq_len, tokenizer):\n",
    "    \n",
    "    packets = rdpcap(pcap_path)\n",
    "    \n",
    "    pad_id, sep_id = sp.PieceToId('[PAD]'),sp.PieceToId('[SEP]')\n",
    "    token = ['[CLS]']\n",
    "    \n",
    "    result = []\n",
    "    i=0\n",
    "    while len(result)!=seq_len: \n",
    "        packet = packets[i]\n",
    "        i+=1\n",
    "        packet_string = packet2string(packet)\n",
    "        \n",
    "        # packet byte string으로 가져와서 tokenizer\n",
    "        if packet_string is None:\n",
    "            print(\"here\")\n",
    "            continue\n",
    "        token.extend(tokenizer.EncodeAsPieces(packet_string))\n",
    "        id = tokenizer.PieceToId(token)\n",
    "        \n",
    "        if(len(id)>70):\n",
    "            id = id[:70]\n",
    "        else:\n",
    "            id.extend([pad_id] * (70 - len(id)))\n",
    "            \n",
    "        id.insert(len(id),sep_id)\n",
    "        result.append(id)\n",
    "    \n",
    "    return result \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no layer\n",
      "here\n",
      "no layer\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "dataset_path = '/home/myk/min0/as'\n",
    "for dir in os.listdir(dataset_path):\n",
    "        path = os.path.join(dataset_path,dir)\n",
    "        for file in os.listdir(path):\n",
    "            r = pcap2token(os.path.join(path,file),10,sp)\n",
    "            dataset.append(r)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([710])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(dataset[0]).flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3180, 7110])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros([len(dataset),7110])\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3180, 710])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros([len(dataset),710])\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    data = dataset[i]\n",
    "    for j in range(len(data)):\n",
    "        t[i] = torch.tensor(data).flatten()\n",
    "        \n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"sentencepiece_10_70.npy\",t.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
